


Tempo / Track 0 globals / MidiEvent List
    This should support TempoEvents. That needs to display and be editable. It needs to import
    and export as well. Must handle multiple Tempo events with absolute positions.
    This needs to inject into the conversion to midi.
    This needs to extract from converion from midi to phrases

Design Solution - This supports the whole execute --> writing thing! right in the grid!: 

Prep work:
    (1) Add four empty rows to start of grid. All grid cells for these 4 rows will start off empty. 
    (2) Add a new text column for Type, read only, with title type. This will be the first visible column
    and other columns will push to the right (higher index)
    (3) Update methods that use the grid to account for (a) the four fixed rows that 
        will always be in the grid at the same position - starting at the first row in the grid.
        (b) the new column - since they are accessed via name mostly, this may not be a problem.


    Add the 4 time-related design elements to the grid
    rename the grid to???
    

    
    The buttons are all phrase (only) buttons - they need to be named and labeled as such
    The grid should auto load from designer
    
    The conversions, both ways need to populate the controls and whatever backing variables they
    have
    The "Phrase" should be replaced by measures which would then allow us to see the alignment
     - measures for phrases will show # of notes in the measure and the start tick e.g.  "14 (2240)       15 (2680) ...."
    The play button will need to allow selection of a block of measures
    
    Needs some method of rearranging the gridrows such as drag and drop or move row up/down arrow.
   
   The grid needs to initialize with the 4 (fixed) control rows, and then phrases add
    starting after. 
    
    What is the first row?  0 or 1? find out!!!






Design  - to manage options per command, like f(multiple phrases) --> new line, etc


ai : Mapping midi to Presonus
Recommended Approach: Extend the Voice Catalog with Mapping Data
The cleanest solution is to extend Voices.Notion.json to include MIDI mappings, 
then use this during both phrase creation and export. This keeps all instrument
knowledge centralized.





piano 2 staffs and drum set : https://chatgpt.com/c/69286cf9-3164-8332-8efa-869b9d1afdd2

=========================  VST STUFF
=================================
Below is a concise assessment and a practical path forward.
Can it work with VST “voice” (instrument) files now? No. The current IMidiPlaybackService + MidiPlaybackService only stream a MIDI file to a hardware/virtual MIDI output device via DryWetMidi. It does not host VST2/VST3 plugins, does not render audio, and has no concept of plugin presets (“voices”).
What is missing for VST support? You need an audio engine and a plugin host layer:
1.	Plugin hosting (VST3 preferred; VST2 is deprecated). A .NET wrapper or interop layer around the VST3 SDK.
2.	Real‑time audio output (WASAPI, ASIO, or CoreAudio). NAudio can help with device abstraction.
3.	MIDI event routing to the plugin (note on/off, CC, program changes).
4.	Voice/instrument catalog mapping (your existing Voices.Notion.json extended with plugin identifiers, preset names, optional key range, transposition).
5.	Latency handling, sample block scheduling, tempo/time signature sync (for plugins needing transport info).
6.	Mixing (if multiple tracks use different plugins) and resource lifecycle (load/unload).
Recommended design evolution Option A: Extend existing interface (simpler but mixes concerns). Option B (cleaner): Keep IMidiPlaybackService for raw MIDI out and add a parallel IVstPlaybackService or a generalized IAudioInstrumentPlaybackService.
Minimal interface extension (if you choose Option A)

You can mix them. Hybrid playback (some parts via external/virtual MIDI device, others via VST instruments) is absolutely doable and common. You do not need to choose “all VST” or “all GM MIDI.” The complexity is manageable if you introduce a clear abstraction for “where notes go.”
Core considerations
1.	Transport/Clock: One scheduler must drive timing for both MIDI-output events and VST-hosted synthesis so they stay in sync.
2.	Latency Alignment: VST audio path has buffer latency; external MIDI devices have near-zero scheduling latency but variable synth response. Add a per-endpoint latency offset if necessary.
3.	Resource Separation: MIDI endpoints just send events; VST endpoints must render audio blocks. Keep rendering thread real-time safe.
4.	Voice Catalog: Extend catalog per part/staff/voice with a rendering mode flag (MidiProgram vs VstPlugin + Preset).
5.	Routing: Each score Part maps to exactly one InstrumentEndpoint; during playback, each timed event is dispatched to that endpoint.
6.	DryWetMidi Playback: Don’t run two independent playbacks. Use DryWetMidi to parse tempo map and timed events, but do custom scheduling so you can dispatch to mixed endpoints uniformly.
Recommended abstraction Define a small interface representing an instrument destination.


==========================================================


Testing:
    Auto Regression:    16 1/8 notes 1 staves - 1 voice
                        12 dotted quarter notes - 1 stave 1 voice
                        18 1/4 note triplets 2 staves - 4 voices
    Tests seems like maybe need to be updated? thats a maybe.


Generate:
       Music can be generated one part at a time, several parts at once, etc. and in small chunks.
        best Strategy to build music may be start with 32nd note rests filling all measures, then combine/replace
        as needed.

        Also sections do not really need to start on beat 1 of measure. Should be more fluid. Like start or 
        end beat can be off +/- 1 beat.
    (Pattern Specific) ParameterDefinitions - control type, data type, range, (dependencies later)

    Pattern - this will be pattern name + the config of the pattern specific parameters - just list of parameters
    Patterns - this is the collection of all available patterns?

    Pattern(Instances) - this will be the instance of a pattern with the standard and
    pattern specific parameter values
    PatternManager - this will manage the loading and saving of patterns and instances

    On the form -- list of available patterns in drop down
    on form  - list of avaiable patterns instances in drop down

    Pattern instances and targets may be separate - so you 
    can apply the same pattern instance to different targets?

Style:
    don't just simulate parts, simulate the artists - style, personality, etc.


===================================================================================================================

AI recommended enhancements:
(1) Recommended section names
Top group (currently Target Scope): Suggested primary: Part & Staff Target Other concise options (pick one):
•	Event Target (Part & Staff)
•	Append Target Selection
•	Part / Staff Selector
•	Voice & Staff Target
•	Destination Part/Staff
Bottom group (currently Splice): Suggested primary: Segment Splice Configuration Other concise options:
•	Score Segment Splicer
•	Splice Builder
•	Segment Merge / Splice
•	Source & Destination Segments
•	Score Range Composer
Rationale: Top describes where new events will be appended (the destination scope). Bottom configures source score ranges and destination for structural operations (splicing/merging). “Segment” clarifies you are operating on partial score ranges (bars/beats).
(2) Potential gaps for complex score authoring / manipulation
Coverage now: You can select parts, staffs, choose bar/beat ranges for two source scores, and a destination score. Missing granularity, musical domains, and safety features common in advanced editors.
Key missing parameters / use cases:
Score/Part targeting:
•	Voice/layer within a staff (e.g., multiple simultaneous voices).
•	Clef choice per staff segment (when splicing cross-clef material).
•	Transposing instrument handling (automatic key / octave adjustment when merging parts).
Temporal precision:
•	Sub-beat resolution (ticks, divisions) for splicing mid-beat.
•	Pickup (anacrusis) and incomplete measure handling.
•	Time signature change boundaries; currently only bars & beats (no validation against measure capacity).
•	Tempo change regions (merging segments should reconcile conflicting tempo maps).
Pitch / key context:
•	Local key changes within the selected segment.
•	Automatic enharmonic normalization when combining segments from different keys.
•	Optional transposition offset (interval / semitone) on splice.
Rhythm / notation integrity:
•	Tie/slur continuation across splice boundaries.
•	Beam group recalculation (to avoid broken beaming after insertion).
•	Tuplet reconciliation (tuplet in source continuing across destination boundary).
•	Grace notes and cue notes preservation.
Structural / navigation aids:
•	Selection by labeled Section/Rehearsal Mark instead of manual bar/beat entry.
•	Multi-segment batch splices (list of ranges rather than only one per source).
•	Preview diff (what measures will be affected) before committing splice.
Content domains not represented:
•	Dynamics (p, mf, cresc) merging rules.
•	Articulations (staccato, accents) integrity across splice.
•	Ornaments (trills, mordents), technical markings (fingering, bowing).
•	Lyrics alignment (syllable shifting if rhythm differs).
•	Chord symbols / harmonic timeline alignment when splicing melody-only or harmony-only materials.
•	Expression / MIDI controller data (if performance layer exists).
Conflict resolution options:
•	Overwrite vs overlay vs skip empty measures.
•	Gap fill (insert rests to match destination time span).
•	Quantize on import (optional rhythmic normalization).
•	Collision policy for existing events (merge, replace, keep both in separate voices).
Validation / feedback:
•	Pre-flight validation (bar length consistency, tuplets balanced, time signature match).
•	Undo/Redo of splice operations.
•	Dry run / simulate splice (show summary in a report textbox).
Usability enhancements:
•	Checkbox to “Apply to all checked staffs” (multi-staff parts like piano).
•	Recent ranges memory (quick reapply).
•	Named range presets (Intro, Verse, Bridge etc.).
•	Color-coded indicator of selected segment validity.
Performance / scalability:
•	Bulk operations (concatenate multiple source scores sequentially).
•	Background processing with progress for large scores.
Data integrity:
•	Key signature and time signature reapplication if destination span empties them.
•	Automatic measure renumbering or retention option.
•	Consistent unique IDs for events after splice (avoid collisions).
Potential new controls:
•	Voice selector: ComboBox (Voice 1..n)
•	Sub-beat selector: NumericUpDown for tick offset
•	Transpose interval: ComboBox (e.g., +2 (M2), -12 (Octave down))
•	Section pickers instead of free-form comboBoxes (bind to existing structural markers)
•	Checkboxes: Preserve Dynamics, Preserve Articulations, Recalculate Beams, Normalize Tuplets
•	Conflict policy: RadioButtons (Overwrite / Merge / Skip)
•	Preview / Validate buttons
Summary: Current design addresses basic part/staff targeting and coarse segment splicing. It lacks finer rhythmic resolution, multi-domain musical symbol handling, structural selection abstractions, conflict policies, and validation—features needed for robust manipulation of complex, professionally notated scores.





==========================================================================================  for a midi show measures as they play event...

  Does the midi player in this application have any type of feedback mechanism that would let me track what tick it's currently playing in order to drive an indicator that shows what percentage of the midi document has played? Do not write code just explain if and how it can be done.

  GitHub Copilot
Yes — you can drive a playback progress indicator. The app already uses Melanchall.DryWetMidi for playback (you create a DryWetMidi Playback via midiDoc.Raw.GetPlayback(...)). That Playback object exposes enough information and events to drive a percent/position indicator. High‑level options and considerations:
1.	Playback state / time polling (simple, reliable)
•	Keep a reference to the Playback instance returned by GetPlayback(...) (your MidiPlaybackService already stores one).
•	Poll Playback.CurrentTime (a TimeSpan) at a UI-friendly interval (e.g. 40–100 ms) with a WinForms Timer. Compute percent = CurrentTime.TotalMilliseconds / midiDoc.Duration.TotalMilliseconds.
•	Update the UI via Invoke/BeginInvoke because polling runs on the UI timer thread or a background timer.
•	Pros: straightforward, stable across tempo maps. Cons: polling interval vs. CPU/latency tradeoff.
2.	Event-driven updates (more precise, lower overhead)
•	DryWetMidi Playback supports events you can subscribe to (e.g. EventPlayed, Finished / Stopped / Started depending on library version). Use those to update the UI when events occur.
•	EventPlayed fires for each MIDI event — you can compute progress based on event time (gives fine-grained updates). Throttle UI updates (don’t update UI for every event).
•	Pros: lower polling, more immediate. Cons: may be very frequent (throttle) and tied to event density.
3.	Tick-level tracking vs. time
•	Playback exposes time as TimeSpan. If you need ticks (MIDI ticks), convert between time and ticks using the song’s TempoMap (tempo map -> time <-> ticks conversions are provided by DryWetMidi APIs).
•	Alternatively, track played events’ absolute tick positions from EventPlayed (it provides event info/absolute time in many API variants). Then compute percent = currentTick / totalTicks where totalTicks comes from the MIDI document (or computed from tempo map + duration).
4.	Total length source
•	Use the MIDI document duration (TimeSpan) or total ticks computed from the raw MIDI + tempo map. Using the same source for numerator and denominator ensures correct percent even with tempo changes.
5.	Threading, UI and latency
•	Always marshal UI updates to the UI thread.
•	Device/output latency and buffering mean the visual position may slightly lead or lag audible output — acceptable for UI progress bars, but important if you need sample-accurate UI sync.
Recommended approach for your app
•	Expose the Playback instance (or a method to query its CurrentTime) from MidiPlaybackService.
•	Use a short WinForms Timer to poll Playback.CurrentTime and compute percent using midiDoc.Duration. Update a progress bar or indicator on the UI thread.
•	If you need higher accuracy or tick-based info, subscribe to Playback.EventPlayed or convert CurrentTime <-> ticks via the MIDI document’s tempo map.
No code provided per request, but that’s the practical options and tradeoffs.

==========================================================================================
