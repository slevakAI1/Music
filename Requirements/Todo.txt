


Tempo / Track 0 globals / MidiEvent List


Design  - to manage options per command, like f(multiple phrases) --> new line, etc


ai : Mapping midi to Presonus
Recommended Approach: Extend the Voice Catalog with Mapping Data
The cleanest solution is to extend Voices.Notion.json to include MIDI mappings, 
then use this during both phrase creation and export. This keeps all instrument
knowledge centralized.





piano 2 staffs and drum set : https://chatgpt.com/c/69286cf9-3164-8332-8efa-869b9d1afdd2

=========================  VST STUFF
=================================
Below is a concise assessment and a practical path forward.
Can it work with VST “voice” (instrument) files now? No. The current IMidiPlaybackService + MidiPlaybackService only stream a MIDI file to a hardware/virtual MIDI output device via DryWetMidi. It does not host VST2/VST3 plugins, does not render audio, and has no concept of plugin presets (“voices”).
What is missing for VST support? You need an audio engine and a plugin host layer:
1.	Plugin hosting (VST3 preferred; VST2 is deprecated). A .NET wrapper or interop layer around the VST3 SDK.
2.	Real‑time audio output (WASAPI, ASIO, or CoreAudio). NAudio can help with device abstraction.
3.	MIDI event routing to the plugin (note on/off, CC, program changes).
4.	Voice/instrument catalog mapping (your existing Voices.Notion.json extended with plugin identifiers, preset names, optional key range, transposition).
5.	Latency handling, sample block scheduling, tempo/time signature sync (for plugins needing transport info).
6.	Mixing (if multiple tracks use different plugins) and resource lifecycle (load/unload).
Recommended design evolution Option A: Extend existing interface (simpler but mixes concerns). Option B (cleaner): Keep IMidiPlaybackService for raw MIDI out and add a parallel IVstPlaybackService or a generalized IAudioInstrumentPlaybackService.
Minimal interface extension (if you choose Option A)

You can mix them. Hybrid playback (some parts via external/virtual MIDI device, others via VST instruments) is absolutely doable and common. You do not need to choose “all VST” or “all GM MIDI.” The complexity is manageable if you introduce a clear abstraction for “where notes go.”
Core considerations
1.	Transport/Clock: One scheduler must drive timing for both MIDI-output events and VST-hosted synthesis so they stay in sync.
2.	Latency Alignment: VST audio path has buffer latency; external MIDI devices have near-zero scheduling latency but variable synth response. Add a per-endpoint latency offset if necessary.
3.	Resource Separation: MIDI endpoints just send events; VST endpoints must render audio blocks. Keep rendering thread real-time safe.
4.	Voice Catalog: Extend catalog per part/staff/voice with a rendering mode flag (MidiProgram vs VstPlugin + Preset).
5.	Routing: Each score Part maps to exactly one InstrumentEndpoint; during playback, each timed event is dispatched to that endpoint.
6.	DryWetMidi Playback: Don’t run two independent playbacks. Use DryWetMidi to parse tempo map and timed events, but do custom scheduling so you can dispatch to mixed endpoints uniformly.
Recommended abstraction Define a small interface representing an instrument destination.


==========================================================


Testing:
    Auto Regression:    16 1/8 notes 1 staves - 1 voice
                        12 dotted quarter notes - 1 stave 1 voice
                        18 1/4 note triplets 2 staves - 4 voices
    Tests seems like maybe need to be updated? thats a maybe.


Generate:
       Music can be generated one part at a time, several parts at once, etc. and in small chunks.
        best Strategy to build music may be start with 32nd note rests filling all measures, then combine/replace
        as needed.

        Also sections do not really need to start on beat 1 of measure. Should be more fluid. Like start or 
        end beat can be off +/- 1 beat.
    (Pattern Specific) ParameterDefinitions - control type, data type, range, (dependencies later)

    Pattern - this will be pattern name + the config of the pattern specific parameters - just list of parameters
    Patterns - this is the collection of all available patterns?

    Pattern(Instances) - this will be the instance of a pattern with the standard and
    pattern specific parameter values
    PatternManager - this will manage the loading and saving of patterns and instances

    On the form -- list of available patterns in drop down
    on form  - list of avaiable patterns instances in drop down

    Pattern instances and targets may be separate - so you 
    can apply the same pattern instance to different targets?

Style:
    don't just simulate parts, simulate the artists - style, personality, etc.


===================================================================================================================

AI recommended enhancements:
(1) Recommended section names
Top group (currently Target Scope): Suggested primary: Part & Staff Target Other concise options (pick one):
•	Event Target (Part & Staff)
•	Append Target Selection
•	Part / Staff Selector
•	Voice & Staff Target
•	Destination Part/Staff
Bottom group (currently Splice): Suggested primary: Segment Splice Configuration Other concise options:
•	Score Segment Splicer
•	Splice Builder
•	Segment Merge / Splice
•	Source & Destination Segments
•	Score Range Composer
Rationale: Top describes where new events will be appended (the destination scope). Bottom configures source score ranges and destination for structural operations (splicing/merging). “Segment” clarifies you are operating on partial score ranges (bars/beats).
(2) Potential gaps for complex score authoring / manipulation
Coverage now: You can select parts, staffs, choose bar/beat ranges for two source scores, and a destination score. Missing granularity, musical domains, and safety features common in advanced editors.
Key missing parameters / use cases:
Score/Part targeting:
•	Voice/layer within a staff (e.g., multiple simultaneous voices).
•	Clef choice per staff segment (when splicing cross-clef material).
•	Transposing instrument handling (automatic key / octave adjustment when merging parts).
Temporal precision:
•	Sub-beat resolution (ticks, divisions) for splicing mid-beat.
•	Pickup (anacrusis) and incomplete measure handling.
•	Time signature change boundaries; currently only bars & beats (no validation against measure capacity).
•	Tempo change regions (merging segments should reconcile conflicting tempo maps).
Pitch / key context:
•	Local key changes within the selected segment.
•	Automatic enharmonic normalization when combining segments from different keys.
•	Optional transposition offset (interval / semitone) on splice.
Rhythm / notation integrity:
•	Tie/slur continuation across splice boundaries.
•	Beam group recalculation (to avoid broken beaming after insertion).
•	Tuplet reconciliation (tuplet in source continuing across destination boundary).
•	Grace notes and cue notes preservation.
Structural / navigation aids:
•	Selection by labeled Section/Rehearsal Mark instead of manual bar/beat entry.
•	Multi-segment batch splices (list of ranges rather than only one per source).
•	Preview diff (what measures will be affected) before committing splice.
Content domains not represented:
•	Dynamics (p, mf, cresc) merging rules.
•	Articulations (staccato, accents) integrity across splice.
•	Ornaments (trills, mordents), technical markings (fingering, bowing).
•	Lyrics alignment (syllable shifting if rhythm differs).
•	Chord symbols / harmonic timeline alignment when splicing melody-only or harmony-only materials.
•	Expression / MIDI controller data (if performance layer exists).
Conflict resolution options:
•	Overwrite vs overlay vs skip empty measures.
•	Gap fill (insert rests to match destination time span).
•	Quantize on import (optional rhythmic normalization).
•	Collision policy for existing events (merge, replace, keep both in separate voices).
Validation / feedback:
•	Pre-flight validation (bar length consistency, tuplets balanced, time signature match).
•	Undo/Redo of splice operations.
•	Dry run / simulate splice (show summary in a report textbox).
Usability enhancements:
•	Checkbox to “Apply to all checked staffs” (multi-staff parts like piano).
•	Recent ranges memory (quick reapply).
•	Named range presets (Intro, Verse, Bridge etc.).
•	Color-coded indicator of selected segment validity.
Performance / scalability:
•	Bulk operations (concatenate multiple source scores sequentially).
•	Background processing with progress for large scores.
Data integrity:
•	Key signature and time signature reapplication if destination span empties them.
•	Automatic measure renumbering or retention option.
•	Consistent unique IDs for events after splice (avoid collisions).
Potential new controls:
•	Voice selector: ComboBox (Voice 1..n)
•	Sub-beat selector: NumericUpDown for tick offset
•	Transpose interval: ComboBox (e.g., +2 (M2), -12 (Octave down))
•	Section pickers instead of free-form comboBoxes (bind to existing structural markers)
•	Checkboxes: Preserve Dynamics, Preserve Articulations, Recalculate Beams, Normalize Tuplets
•	Conflict policy: RadioButtons (Overwrite / Merge / Skip)
•	Preview / Validate buttons
Summary: Current design addresses basic part/staff targeting and coarse segment splicing. It lacks finer rhythmic resolution, multi-domain musical symbol handling, structural selection abstractions, conflict policies, and validation—features needed for robust manipulation of complex, professionally notated scores.





